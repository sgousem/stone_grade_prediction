{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6806 images belonging to 5 classes.\n",
      "WARNING:tensorflow:From <ipython-input-1-adbf84350a58>:78: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 53 steps\n",
      "Epoch 1/25\n",
      "53/53 [==============================] - 322s 6s/step - loss: 0.9302 - accuracy: 0.5684\n",
      "Epoch 2/25\n",
      "53/53 [==============================] - 270s 5s/step - loss: 0.5141 - accuracy: 0.7800\n",
      "Epoch 3/25\n",
      "53/53 [==============================] - 250s 5s/step - loss: 0.3728 - accuracy: 0.8362\n",
      "Epoch 4/25\n",
      "53/53 [==============================] - 250s 5s/step - loss: 0.3677 - accuracy: 0.8408\n",
      "Epoch 5/25\n",
      "53/53 [==============================] - 245s 5s/step - loss: 0.2998 - accuracy: 0.8730\n",
      "Epoch 6/25\n",
      "53/53 [==============================] - 251s 5s/step - loss: 0.2719 - accuracy: 0.8871\n",
      "Epoch 7/25\n",
      "53/53 [==============================] - 269s 5s/step - loss: 0.2413 - accuracy: 0.8983\n",
      "Epoch 8/25\n",
      "53/53 [==============================] - 258s 5s/step - loss: 0.2038 - accuracy: 0.9220\n",
      "Epoch 9/25\n",
      "53/53 [==============================] - 267s 5s/step - loss: 0.1383 - accuracy: 0.9461\n",
      "Epoch 10/25\n",
      "53/53 [==============================] - 257s 5s/step - loss: 0.1229 - accuracy: 0.9546\n",
      "Epoch 11/25\n",
      "53/53 [==============================] - 207s 4s/step - loss: 0.0926 - accuracy: 0.9684\n",
      "Epoch 12/25\n",
      "53/53 [==============================] - 208s 4s/step - loss: 0.0898 - accuracy: 0.9681\n",
      "Epoch 13/25\n",
      "53/53 [==============================] - 212s 4s/step - loss: 0.0575 - accuracy: 0.9786\n",
      "Epoch 14/25\n",
      "53/53 [==============================] - 222s 4s/step - loss: 0.1037 - accuracy: 0.9635\n",
      "Epoch 15/25\n",
      "53/53 [==============================] - 211s 4s/step - loss: 0.0630 - accuracy: 0.9763\n",
      "Epoch 16/25\n",
      "53/53 [==============================] - 203s 4s/step - loss: 0.0412 - accuracy: 0.9862\n",
      "Epoch 17/25\n",
      "53/53 [==============================] - 208s 4s/step - loss: 0.0319 - accuracy: 0.9904\n",
      "Epoch 18/25\n",
      "53/53 [==============================] - 213s 4s/step - loss: 0.0282 - accuracy: 0.9913\n",
      "Epoch 19/25\n",
      "53/53 [==============================] - 209s 4s/step - loss: 0.0226 - accuracy: 0.9922\n",
      "Epoch 20/25\n",
      "53/53 [==============================] - 210s 4s/step - loss: 0.1307 - accuracy: 0.9602\n",
      "Epoch 21/25\n",
      "53/53 [==============================] - 206s 4s/step - loss: 0.0397 - accuracy: 0.9859\n",
      "Epoch 22/25\n",
      "53/53 [==============================] - 208s 4s/step - loss: 0.0284 - accuracy: 0.9904\n",
      "Epoch 23/25\n",
      "53/53 [==============================] - 211s 4s/step - loss: 0.0195 - accuracy: 0.9942\n",
      "Epoch 24/25\n",
      "53/53 [==============================] - 212s 4s/step - loss: 0.0320 - accuracy: 0.9882\n",
      "Epoch 25/25\n",
      "53/53 [==============================] - 224s 4s/step - loss: 0.0694 - accuracy: 0.9745\n"
     ]
    }
   ],
   "source": [
    "def train_CNN(train_directory,target_size=(200,200), classes=None,\n",
    "              batch_size=128,num_epochs=20,num_classes=5,verbose=0):\n",
    "    \"\"\"\n",
    "    Trains a conv net for the flowers dataset with a 5-class classifiction output\n",
    "    Also provides suitable arguments for extending it to other similar apps\n",
    "    \n",
    "    Arguments:\n",
    "            train_directory: The directory where the training images are stored in separate folders.\n",
    "                            These folders should be named as per the classes.\n",
    "            target_size: Target size for the training images. A tuple e.g. (200,200)\n",
    "            classes: A Python list with the classes \n",
    "            batch_size: Batch size for training\n",
    "            num_epochs: Number of epochs for training\n",
    "            num_classes: Number of output classes to consider\n",
    "            verbose: Verbosity level of the training, passed on to the `fit_generator` method\n",
    "    Returns:\n",
    "            A trained conv net model\n",
    "    \n",
    "    \"\"\"\n",
    "    from keras.preprocessing.image import ImageDataGenerator\n",
    "    import tensorflow as tf\n",
    "    from keras.optimizers import RMSprop\n",
    "    \n",
    "    # ImageDataGenerator object instance with scaling\n",
    "    train_datagen = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "    # Flow training images in batches using the generator\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "            train_directory,  # This is the source directory for training images\n",
    "            target_size=target_size,  # All images will be resized to 200 x 200\n",
    "            batch_size=batch_size,\n",
    "            # Specify the classes explicitly\n",
    "            classes = classes,\n",
    "            # Since we use categorical_crossentropy loss, we need categorical labels\n",
    "            class_mode='categorical')\n",
    "    \n",
    "    input_shape = tuple(list(target_size)+[3])\n",
    "    \n",
    "    # Model architecture\n",
    "    model = tf.keras.models.Sequential([\n",
    "    # Note the input shape is the desired size of the image 200x 200 with 3 bytes color\n",
    "    # The first convolution\n",
    "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=input_shape),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    # The second convolution\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # The third convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # The fourth convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # The fifth convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # Flatten the results to feed into a dense layer\n",
    "    tf.keras.layers.Flatten(),\n",
    "    # 512 neuron in the fully-connected layer\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    # 5 output neurons for 5 classes with the softmax activation\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Optimizer and compilation\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    # Total sample count\n",
    "    total_sample=train_generator.n\n",
    "    \n",
    "    # Training\n",
    "    model.fit_generator(\n",
    "        train_generator, \n",
    "        steps_per_epoch=int(total_sample/batch_size),  \n",
    "        epochs=num_epochs,\n",
    "        verbose=verbose)\n",
    "    \n",
    "    return model\n",
    "train_directory = \"stone_new\"\n",
    "trained_model=train_CNN(train_directory=train_directory,classes=['mm_10','mm_20','mm_265','sand_plast','sand_top'],\n",
    "                        num_epochs=25,num_classes=5,verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'flower-test/rose2.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-0605ac4724c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtest_image\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'flower-test/rose2.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m#img_cup=Image.open('../Data/101_ObjectCategories/Test_images/cup-1.jpg')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#test_image_cup=Image.open('../Data/101_ObjectCategories/Test_images/crab-cup-1.jpg')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode)\u001b[0m\n\u001b[0;32m   2764\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2765\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2766\u001b[1;33m         \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2767\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2768\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'flower-test/rose2.jpg'"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "test_image=Image.open('flower-test/rose2.jpg')\n",
    "#img_cup=Image.open('../Data/101_ObjectCategories/Test_images/cup-1.jpg')\n",
    "#test_image_cup=Image.open('../Data/101_ObjectCategories/Test_images/crab-cup-1.jpg')\n",
    "test_image = test_image.resize((200,200))\n",
    "#img_cup = img_cup.resize((200,200))\n",
    "#test_image_cup = test_image_cup.resize((200,200))\n",
    "test_image=np.expand_dims(test_image,axis=0)\n",
    "#img_cup=np.expand_dims(img_cup,axis=0)\n",
    "#test_image_cup=np.expand_dims(test_image_cup,axis=0)\n",
    "predict_class = trained_model.predict(test_image)\n",
    "print(type(predict_class))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model.save('cnn_stone_new.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
